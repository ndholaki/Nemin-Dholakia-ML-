---
title: "Machine Learning 3"
author: "Nemin Dholakia"
date: "10/18/2021"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
library("reshape2")
library("dplyr")
library("tidyr")
library("ggplot2")
library("ROCR")
library("rpart")
library("rpart.plot")
library("caret")
library("randomForest")
library("tidyverse")
library("tm")
library("SnowballC")
library("softImpute")
library("glmnet")
library("Hmisc")
library("dummies")
library('tinytex')
library('GGally')
library('gplots')
library('FNN')
library("dplyr")
library("tidyr")
library("caTools")
library("ggpubr")
library("e1071")
```
```{r}
rm(list=ls())
bank_data <- read_csv("UniversalBank (1).csv")
View(bank_data)

```

```{r}
bank_data <- read.csv("UniversalBank (1).csv")
bank_data$Personal.Loan = as.factor(bank_data$Personal.Loan)
bank_data$Online = as.factor(bank_data$Online)
bank_data$CreditCard = as.factor(bank_data$CreditCard)
set.seed(1)
train.index <- sample(row.names(bank_data), 0.6*dim(bank_data)[1])  
test.index <- setdiff(row.names(bank_data), train.index) 
train.df <- bank_data[train.index, ]
test.df <- bank_data[test.index, ]
train <- bank_data[train.index, ]
test = bank_data[train.index,]
```



###a. Creating a pivot table for the training data with Online as a column variable, CC as a row variable, and Loan as a secondary row variable. The values inside the table should convey the count. In R use functions melt() and cast(), or function table().

```{r}
melted.bank_data = melt(train,id=c("CreditCard","Personal.Loan"),variable= "Online")
recast.bank_data=dcast(melted.bank_data,CreditCard+Personal.Loan~Online)
recast.bank_data[,c(1:2,14)]
```


###b. Considering the task of classifying a customer who owns a bank credit card and is actively using online banking services. Looking at the pivot table, what is the probability that this customer will accept the loan offer? [This is the probability of loan acceptance (Loan = 1) conditional on having a bank credit card (CC = 1) and being an active user of online banking services (Online = 1)].

####Probability of Loan acceptance given having a bank credit card and user of online services is 77/3000 = 2.6%


###c. Create two separate pivot tables for the training data. One will have Loan (rows) as a function of Online (columns) and the other will have Loan (rows) as a function of CC.
```{r}
melted.bank_datac1 = melt(train,id=c("Personal.Loan"),variable = ("Online"))
recast.bank_datac1=dcast(melted.bank_data,Personal.Loan~Online)
recast.bank_datac1[,c(1:2,13)]
```

```{r}
melted.bank_datac2 = melt(train,id=c("CreditCard"),variable = "Online")
recast.bank_datac2=dcast(melted.bank_data,CreditCard~Online)
recast.bank_datac2[,c(1:2,13)]
```

```{r}
recast.bank_datac1=dcast(melted.bank_datac1,Personal.Loan~Online)
recast.bank_datac2=dcast(melted.bank_datac2,CreditCard~Online)
RelLoanline=recast.bank_datac1[,c(1,13)]
RelLoanCC = recast.bank_datac2[,c(1,14)]
RelLoanline
```
```{r}
RelLoanCC
```


###d. Computing the following quantities [P (A | B) means “the probability of A given B”]:
(i) P (CC = 1 | Loan = 1) (the proportion of credit card holders among the loan acceptors)
(ii) P(Online=1|Loan=1)
(iii) P (Loan = 1) (the proportion of loan acceptors)
(iv) P(CC=1|Loan=0)
(v) P(Online=1|Loan=0)
(vi) P(Loan=0)
```{r}
table(train[,c(14,10)])
```
```{r}
table(train[,c(13,10)])
```
```{r}
table(train[,c(10)])
```



i. 77/(77+198)=28%
ii. 166/(166+109)= 60.3%
iii.275/(275+2725)=9.2%
iv. 801/(801+1924)=29.4%
v. 1588/(1588+1137) = 58.3%
vi. 2725/(2725+275) = 90.8%
###e. Using the quantities computed above to compute the naive Ba1 probability P(Loan = 1 | CC = 1, Online = 1).
```{r}
((77/(77+198))*(166/(166+109))*(275/(275+2725)))/(((77/(77+198))*(166/(166+109))*(275/(275+2725)))+((801/(801+1924))*(1588/(1588+1137))*2725/(2725+275)))
```


###f. Comparing this value with the one obtained from the pivot table in (b). Which is a more accurate estimate? 9.05% are very similar to the 9.7% the difference between the exact method and the naive-baise method is the exact method would need the the exact same independent variable classifications to predict, where the naive bayes method does not.

###g. The entries in this table are needed for computing P (Loan = 1 | CC = 1, Online = 1)? In R, run naive Bayes on the data. Examine the model output on training data, and find the entry that corresponds to P (Loan = 1 | CC = 1, Online = 1). Compare this to the number you obtained in (e).
```{r}
naive.train = train.df[,c(10,13:14)]
naive.test = test.df[,c(10,13:14)]
naivebayes = naiveBayes(Personal.Loan~.,data=naive.train)
naivebayes
```


##the naive bayes is the exact same output we recieved in the previous methods.
###The same response provided as above (.280)(.603)(.09)/(.280.603.09+.29.58.908) = .09 

